{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 4. Конструирование текстовых признаков из твитов пользователей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сбор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый этап - сбор твитов пользователей. Необходимо подключаться к Twitter API и запрашивать твиты по id пользователя. \n",
    "Подключение к API подробно описано в ДЗ 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "\n",
    "CONSUMER_KEY = \"\"\n",
    "CONSUMER_SECRET = \"\"\n",
    "\n",
    "ACCESS_TOKEN_KEY = \"\"\n",
    "ACCESS_TOKEN_SECRET = \"\"\n",
    "\n",
    "api = twitter.Api(consumer_key=CONSUMER_KEY, \n",
    "                  consumer_secret=CONSUMER_SECRET, \n",
    "                  access_token_key=ACCESS_TOKEN_KEY, \n",
    "                  access_token_secret=ACCESS_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения твитов пользователя может быть использован метод GetUserTimeline из библиотеки python-twitter. Он позволяет получить не более 200 твитов пользователя.\n",
    "\n",
    "Метод имеет ограничение по количеству запросов в секунду. Для получения информации о промежутке времени, которое необходимо подождать для повторного обращения к API может быть использован метод `GetSleepTime`. Для получения информации об ограничениях запросов с помощью метода `GetUserTimeLine` необходимо вызывать `GetSleepTime` с параметром \"statuses/user_timeline\".\n",
    "\n",
    "Метод GetUserTimeline возвращает объекты типа Status. У этих объектов есть метод AsDict, который позволяет представить твит в виде словаря.\n",
    "\n",
    "Id пользователей необходимо считать из файла, как было сделано в ДЗ 1.\n",
    "\n",
    "Необходимо реализовать функцию `get_user_tweets(user_id)`. Входной параметр - id пользователя из файла. Возвращаемое значение - массив твитов пользователя, где каждый твит представлен в виде словаря. Предполагается, что информация о пользователе содержится в твитах, которые пользователь написал сам. Это означает, что можно попробовать отфильтровать ответы другим пользователям, ссылки и ретвиты, а так же картинки и видео, так как наша цель - найти текстовую информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_user_tweets(user_id):\n",
    "    print \"Processing user \" + str(user_id)\n",
    "    all_tweets = []\n",
    "    statuses = None\n",
    "    while statuses is None:\n",
    "        try:\n",
    "            statuses = api.GetUserTimeline(user_id, count=200, include_rts=False,\n",
    "                                                   trim_user=True, exclude_replies=True)\n",
    "        except Exception as e:\n",
    "            print e\n",
    "            sleep_time = api.GetSleepTime(\"statuses/user_timeline\")\n",
    "            if sleep_time > 0:\n",
    "                print \"Going to sleep \" + str(sleep_time) + \" seconds\"\n",
    "                time.sleep(sleep_time)\n",
    "            else:       \n",
    "                print \"PROBLEM with \" + str(user_id)\n",
    "                return []\n",
    "        \n",
    "    all_tweets = statuses\n",
    "    all_tweets = [tweet.AsDict()['text'] for tweet in all_tweets]\n",
    "    print \"Tweets for \" + str(user_id) + \" is \" + str(len(all_tweets))\n",
    "    return all_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбор текста твита"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка текста предполагает разбиение текста на отдельные элементы - параграфы, предложения, слова. Мы будем преобразовывать текст твита к словам. Для этого текст необходимо разбить на слова. Сделать это можно, например, с помощью регулярного выражения.\n",
    "\n",
    "Необходимо реализовать функцию, `get_words(text)`. Входной параметр - строка с текстом. Возвращаемое значение - массив строк (слов). Обратите внимание, что нужно учесть возможное наличие пунктуации и выделить по возможности только слова. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk.tokenize.casual\n",
    "import string\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "punctuation = string.punctuation\n",
    "\n",
    "REGEXPS = (\n",
    "    nltk.tokenize.casual.URLS,\n",
    "    # ASCII Emoticons\n",
    "    nltk.tokenize.casual.EMOTICONS\n",
    "    ,\n",
    "    # HTML tags:\n",
    "    r\"\"\"<[^>\\s]+>\"\"\"\n",
    "    ,\n",
    "    # ASCII Arrows\n",
    "    r\"\"\"[\\-]+>|<[\\-]+\"\"\"\n",
    "    ,\n",
    "    # Ellipsis dots.\n",
    "    r\"\"\"(?:\\.(?:\\s*\\.){1,})\"\"\"\n",
    "    ,\n",
    "    # Numbers, including fractions, decimals.\n",
    "    r\"\"\"(?:[+\\-]?\\d+([,/.:-]\\d+[+\\-]?)?)\"\"\"\n",
    "    ,\n",
    "    #Twitter hashtags\n",
    "    r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    ")\n",
    "\n",
    "PURE_RE = re.compile(r\"\"\"(%s)\"\"\" % \"|\".join(REGEXPS), re.VERBOSE | re.I | re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_words(text):\n",
    "    \"\"\"returns list of words\"\"\"\n",
    "    # your code here\n",
    "    words = [PURE_RE.sub(\"\", token) for token in tokenizer.tokenize(text) if token not in punctuation]\n",
    "    return [word for word in words if word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее полученные слова необходимо привести к нормальной форме. То есть привести их к форме единственного числа настоящего времени и пр. Сделать это можно с помощью библиотеки nltk. Информацию по загрузке, установке библиотеки и примерах использования можно найти на сайте http://www.nltk.org/\n",
    "\n",
    "Для загрузки всех необходимых словарей можно воспользоваться методом download из библиотеки nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://www.nltk.org/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для дальнейшей обработки слова должны быть приведены к нижнему регистру. \n",
    "\n",
    "Для приведения к нормальной форме можно использовать `WordNetLemmatizer` из библиотеки nltk. У этого класса есть метод `lemmatize`.\n",
    "\n",
    "Также необходимо убрать из текста так называемые стоп-слова. Это часто используемые слова, не несущие смысловой нагрузки для наших задач. Сделать это можно с помощью `stopwords` из nltk.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать функцию `get_tokens(words)`. Входной параметр - массив слов. Возвращаемое значение - массив токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tokens(words):\n",
    "    \"\"\"returns list of tokens\"\"\"\n",
    "    # your code here\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "    tokens = [word for word in lemmatized_words if word not in stopwords.words('english')]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать функцию `get_tweet_tokens(tweet)`. Входной параметр - текст твита. Возвращаемое значение -- токены твита. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tweet_tokens(tweet):\n",
    "    # your code here\n",
    "    words = get_words(tweet)\n",
    "    tokens = get_tokens(words)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать функцию `collect_users_tokens()`. Функция должна сконструировать матрицу признаков пользователей. В этой матрице строка - пользователь. Столбец - токен. На пересечении - сколько раз токен встречается у пользователя.\n",
    "Для построения матрицы можно использовать `DictVectorizer` из `sklearn.feature_extraction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_users_tokens(df_users, tweet_collection):\n",
    "    \"\"\"returns users list and list of user dicts. Each dict contains frequence of user tokens\"\"\"\n",
    "    # your code here\n",
    "    users = []\n",
    "    user_tokens = []\n",
    "    \n",
    "    tokens = {}\n",
    "    for user in df_users[\"twitter_id\"]:\n",
    "        users.append(str(user))\n",
    "        for tweet in tweet_collection[str(user)]:\n",
    "            tweet_tokens = get_tweet_tokens(tweet)\n",
    "            for token in tweet_tokens:\n",
    "                if token in tokens.keys():\n",
    "                    tokens[token] += 1\n",
    "                else:\n",
    "                    tokens[token] = 1\n",
    "        user_tokens.append(tokens)\n",
    "        tokens = {}\n",
    "        \n",
    "    return users, user_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twitter_id</th>\n",
       "      <th>is_1</th>\n",
       "      <th>is_2</th>\n",
       "      <th>is_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66412773</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10143902</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73701917</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82209363</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47063951</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   twitter_id  is_1  is_2  is_3\n",
       "0    66412773     0     0     1\n",
       "1    10143902     0     0     1\n",
       "2    73701917     0     0     1\n",
       "3    82209363     0     0     1\n",
       "4    47063951     0     0     1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "TRAINING_SET_URL = \"twitter_train.csv\"\n",
    "df_users = pd.read_csv(TRAINING_SET_URL, sep=\",\", header=0, names=[\"twitter_id\", \"is_1\", \"is_2\", \"is_3\"])\n",
    "df_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "users = df_users['twitter_id'].values\n",
    "user_chunks = np.split(users, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_tweets = {}\n",
    "chunk_num = 15\n",
    "\n",
    "while chunk_num < len(user_chunks):\n",
    "    users = user_chunks[chunk_num]\n",
    "    print \"Proccessing chunk \", chunk_num\n",
    "    for twitter_id in users:\n",
    "        user_tweets[twitter_id] = get_user_tweets(twitter_id)\n",
    "    \n",
    "    f = open(\"tweets_\" + str(chunk_num) + \".txt\", \"w\")\n",
    "    json.dump(user_tweets, f)\n",
    "    user_tweets = {}\n",
    "    f.close()\n",
    "    chunk_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_collection = {}\n",
    "\n",
    "#Merge chunks with user tweets\n",
    "for i in xrange(100):\n",
    "    f = open(\"short_tweets/tweets_\" + str(i) + \".txt\")\n",
    "    users = json.load(f)\n",
    "    for key in users.keys():\n",
    "        tweet_collection[key] = users[key]\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users, users_tokens = collect_users_tokens(df_users, tweet_collection)\n",
    "v = DictVectorizer()\n",
    "vs = v.fit_transform(users_tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Сохраним полученные данные в файл. Используется метод savez из numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savez(\"out_4.dat\", data=vs, users=users, users_tokens=users_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее для получения представления о полученной информацию о токенах предлагается отобразить ее в виде облака тэгов. [Подсказка](http://anokhin.github.io/img/tag_cloud.png). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_tag_cloud(v, vs):\n",
    "    \"\"\"Draws tag cloud of found tokens\"\"\"\n",
    "    # your code here\n",
    "    filename = \"cloud.png\"\n",
    "    frequency = np.asarray(vs.sum(axis=0))\n",
    "    frequency = frequency.reshape(frequency.shape[1])\n",
    "    n = np.sum(frequency)\n",
    "    frequency = frequency / n\n",
    "    words = v.get_feature_names()\n",
    "    \n",
    "    freqs = []\n",
    "    for i, word in enumerate(words):\n",
    "        freqs.append((word, frequency[i]))\n",
    "    \n",
    "    freqs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    cloud = wordcloud.WordCloud(width=1920, height=1080, max_words=1000)\n",
    "    cloud.fit_words(freqs)\n",
    "    cloud.to_file(filename)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "draw_tag_cloud(v, vs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
